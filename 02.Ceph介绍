# Ceph

##### Ceph简介


Ceph使用C++语言开发，遵循LGPL协议开源。Sage Weil(Ceph论文发表者)于2011年创立了以Inktank公司主导Ceph的开发和社区维护。
2014年Redhat收购inktank公司，并发布Inktank Ceph企业版（ICE）软件，业务场景聚焦云、备份和归档，支持对象存储和块存储以及文件系统存储应用。
出现Ceph开源社区版本和Redhat企业版。

Cphe主要设计的初衷是变成一个可避免单节点故障的分布式文件系统，PB级别的扩展能力，而且是一种开源自由软件，许多超融合的分布式文件系统都是基于Ceph开发的。


Ceph是一个统一的分布式存储系统，设计初衷是提供较好的性能、可靠性和可扩展性。


##### ceph的优势


高扩展性：使用普通x86服务器，支持10~1000台服务器，支持TB到EB级的扩展。
高可靠性：没有单点故障，多数据副本，自动管理，自动修复。
高性能：数据分布均衡。
可用于对象存储，块设备存储和文件系统存储


### ceph架构

基础存储系统
rados：基础存储系统RADOS（Reliable, Autonomic, Distributed Object Store，即可靠的、自动化的、分布式的对象存储）。
所有存储在Ceph系统中的用户数据事实上最终都是由这一层来存储的。
Ceph的高可靠、高可扩展、高性能、高自动化等等特性本质上也是由这一层所提供的。

基础库librados:
librados：这一层的功能是对RADOS进行抽象和封装，并向上层提供API，以便直接基于RADOS（而不是整个Ceph）进行应用开发。
特别要注意的是，RADOS是一个对象存储系统，因此，librados实现的API也只是针对对象存储功能的。

高层应用接口
radosgw：对象网关接口(对象存储)
rbd:块存储
cephfs：文件系统存储
其作用是在librados库的基础上提供抽象层次更高、更便于应用或客户端使用的上层接口。




#### Ceph的基本组件

Ceph主要有三个基本进程

Osd
用于集群中所有数据与对象的存储。处理集群数据的复制、恢复、回填、再均衡。并向其他osd守护进程发送心跳，然后向Mon提供一些监控信息。
当Ceph存储集群设定数据有两个副本时（一共存两份），则至少需要两个OSD守护进程即两个OSD节点，集群才能达到active+clean状态。

MDS(可选)
为Ceph文件系统提供元数据计算、缓存与同步（也就是说，Ceph 块设备和 Ceph 对象存储不使用MDS ）。
在ceph中，元数据也是存储在osd节点中的，mds类似于元数据的代理缓存服务器。MDS进程并不是必须的进程，只有需要使用CEPHFS时，才需要配置MDS节点。

Monitor
监控整个集群的状态，维护集群的cluster MAP二进制表，保证集群数据的一致性。
ClusterMAP描述了对象块存储的物理位置，以及一个将设备聚合到物理位置的桶列表。

Manager（ceph-mgr）
用于收集ceph集群状态、运行指标，比如存储利用率、当前性能指标和系统负载。
对外提供 ceph dashboard（ceph ui）和 resetful api。Manager组件开启高可用时，至少2个


##### ceph 结构包含两个部分


ceph client：访问 ceph 底层服务或组件，对外提供各种接口。比如：对象存储接口、块存储接口、文件级存储接口。
ceph node：ceph 底层服务提供端，也就是 ceph 存储集群。


#### Ceph存储种类及其应用场景

##### 块存储

典型设备： 磁盘阵列，硬盘

主要是将裸磁盘空间映射给主机使用的。

优点：

通过Raid与LVM等手段，对数据提供了保护。
多块廉价的硬盘组合起来，提高容量。
多块磁盘组合出来的逻辑盘，提升读写效率。

缺点：
采用SAN架构组网时，光纤交换机，造价成本高。
主机之间无法共享数据。

使用场景：

Docker容器、虚拟机磁盘存储分配。
日志存储。
文件存储。


##### 文件存储

典型设备： FTP、NFS服务器
为了克服块存储文件无法共享的问题，所以有了文件存储。
在服务器上架设FTP与NFS服务，就是文件存储。

优点：

造价低，随便一台机器就可以了。
方便文件共享。

缺点：

读写速率低。
传输速率慢。

使用场景：

日志存储。
有目录结构的文件存储。


##### 对象存储


为什么需要对象存储？

首先，一个文件包含了属性（术语叫metadata，元数据，例如该文件的大小、修改时间、存储路径等）以及内容（以下简称数据）。

例如FAT32这种文件系统，存储过程是链表的形式。

而对象存储则将元数据独立了出来，控制节点叫元数据服务器（服务器+对象存储管理软件），里面主要负责存储对象的属性（主要是对象的数据被打散存放到了那几台分布式服务器中的信息），而其他负责存储数据的分布式服务器叫做OSD，主要负责存储文件的数据部分。
当用户访问对象，会先访问元数据服务器，元数据服务器只负责反馈对象存储在哪些OSD，假设反馈文件A存储在B、C、D三台OSD，那么用户就会再次直接访问3台OSD服务器去读取数据。

这时候由于是3台OSD同时对外传输数据，所以传输的速度就加快了。当OSD服务器数量越多，这种读写速度的提升就越大，通过此种方式，实现了读写快的目的。

另一方面，对象存储软件是有专门的文件系统的，所以OSD对外又相当于文件服务器，那么就不存在文件共享方面的困难了，也解决了文件共享方面的问题。

所以对象存储的出现，很好地结合了块存储与文件存储的优点。

优点：

具备块存储的读写高速。
具备文件存储的共享等特性。

使用场景： (适合更新变动较少的数据)
图片存储。
视频存储。


**Ceph同时提供对象存储、块存储和文件系统存储三种功能，满足不同应用需求。**

###### 扩展什么是OSD


对象存储（Object-based Storage)是一种新的网络存储架构，基于对象存储技术的设备就是对象存储设备（Object-based Storage Device）简称OSD。
总体上来讲，对象存储综合了NAS和SAN的优点，同时具有SAN的高速直接访问和NAS的分布式数据共享等优势，提供了具有高性能、高可靠性、跨平台以及安全的数据共享的存储体系结构。


## Ceph 工作原理

##### ceph数据的存储过程

无论使用哪种存储方式（对象、块、挂载），存储的数据都会被切分成对象（Objects）。Objects size大小可以由管理员调整，通常为2M或4M。
每个对象都会有一个唯一的OID，由ino与ono生成，虽然这些名词看上去很复杂，其实相当简单。ino即是文件的File ID，用于在全局唯一标示每一个文件，而ono则是分片的编号。
比如：一个文件FileID为A，它被切成了两个对象，一个对象编号0，另一个编号1，那么这两个文件的oid则为A0与A1。
Oid的好处是可以唯一标示每个不同的对象，并且存储了对象与文件的从属关系。由于ceph的所有数据都虚拟成了整齐划一的对象，所以在读写时效率都会比较高。

但是对象并不会直接存储进OSD中，因为对象的size很小，在一个大规模的集群中可能有几百到几千万个对象。这么多对象光是遍历寻址，速度都是很缓慢的；
并且如果将对象直接通过某种固定映射的哈希算法映射到osd上，当这个osd损坏时，对象无法自动迁移至其他osd上面（因为映射函数不允许）。
为了解决这些问题，ceph引入了归置组的概念，即PG。

PG是一个逻辑概念，我们linux系统中可以直接看到对象，但是无法直接看到PG。
它在数据寻址时类似于数据库中的索引：每个对象都会固定映射进一个PG中，所以当我们要寻找一个对象时，只需要先找到对象所属的PG，然后遍历这个PG就可以了，无需遍历所有对象。
而且在数据迁移时，也是以PG作为基本单位进行迁移，ceph不会直接操作对象。

对象时如何映射进PG的？还记得OID么？首先使用静态hash函数对OID做hash取出特征码，用特征码与PG的数量去模，得到的序号则是PGID。
由于这种设计方式，PG的数量多寡直接决定了数据分布的均匀性，所以合理设置的PG数量可以很好的提升CEPH集群的性能并使数据均匀分布。

最后PG会根据管理员设置的副本数量进行复制，然后通过crush算法存储到不同的OSD节点上（其实是把PG中的所有对象存储到节点上），第一个osd节点即为主节点，其余均为从节点。



### Ceph名词介绍


Ceph核心组件及概念介绍

Monitor

监控整个集群的状态，维护集群的cluster MAP二进制表，保证集群数据的一致性

OSD --

OSD全称Object Storage Device，也就是负责响应客户端请求返回具体数据的进程。一个Ceph集群一般都有很多个OSD。

MDS

MDS全称Ceph Metadata Server，是CephFS服务依赖的元数据服务。

Object --

Ceph最底层的存储单元是Object对象，每个Object包含元数据和原始数据。

PG --

PG全称Placement Grouops，是一个逻辑的概念，一个PG包含多个OSD。引入PG这一层其实是为了更好的分配数据和定位数据。

RADOS

RADOS全称Reliable Autonomic Distributed Object Store，是Ceph集群的精华，用户实现数据分配、Failover等集群操作。

Libradio

Librados是Rados提供库，因为RADOS是协议很难直接访问，因此上层的RBD、RGW和CephFS都是通过librados访问的，目前提供PHP、Ruby、Java、Python、C和C++支持。

CRUSH --

CRUSH是Ceph使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方。

RBD --

RBD全称RADOS block device，是Ceph对外提供的块设备服务。

RGW

RGW全称RADOS gateway，是Ceph对外提供的对象存储服务，接口与S3和Swift兼容。

CephFS --

CephFS全称Ceph File System，是Ceph对外提供的文件系统服务。
